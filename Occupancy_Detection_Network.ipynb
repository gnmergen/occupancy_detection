{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Occupancy Detection Network",
      "provenance": [],
      "authorship_tag": "ABX9TyMJ4JyDohmFugZ0GrkEk6RB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnmergen/occupancy_detection/blob/main/Occupancy_Detection_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Packages Used:"
      ],
      "metadata": {
        "id": "ZMDcG_pHvRnN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxnVIhEQVVgQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turned out we didn't need gpu\n",
        "cuda0 = torch.device('cuda:0')  # pick the GPU at index 0"
      ],
      "metadata": {
        "id": "_gTJrX2gV5ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing into NN friendly format\n",
        "\n",
        "occupancy = pd.read_csv('occupancy_data.csv')\n",
        "occupancy = occupancy.drop('date', axis = 1)\n",
        "occupancy['Occupancy'] = occupancy['Occupancy'].astype(object)"
      ],
      "metadata": {
        "id": "D4ytvogqmNGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = occupancy.drop('Occupancy', axis = 1), occupancy['Occupancy']"
      ],
      "metadata": {
        "id": "iAEItUy4mUT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.20) # splitting data"
      ],
      "metadata": {
        "id": "kX7UWF0jm0ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler() # normalizing predictor variables\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "MGBIrA4vWlnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.to_numpy()"
      ],
      "metadata": {
        "id": "Tdft6noApmqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = y_test.to_numpy()"
      ],
      "metadata": {
        "id": "6wXBSWJap6qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, input_dimension):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dimension, 32)\n",
        "    self.fc2 = nn.Linear(32, 32)\n",
        "    self.fc3 = nn.Linear(32, 16)\n",
        "    self.fc4 = nn.Linear(16, 1)\n",
        "\n",
        "  def forward(self, input_dimension):\n",
        "    input_dimension = self.fc1(input_dimension)\n",
        "    input_dimension = input_dimension.relu()\n",
        "    input_dimension = self.fc2(input_dimension)\n",
        "    input_dimension = input_dimension.relu()\n",
        "    input_dimension = self.fc3(input_dimension)\n",
        "    input_dimension = input_dimension.relu()\n",
        "    return self.fc4(input_dimension)\n",
        "\n",
        "_, input_dimension = X_train.shape\n",
        "model = Classifier(input_dimension)"
      ],
      "metadata": {
        "id": "mPn7br1EXp1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_loss_function():\n",
        "  return nn.BCEWithLogitsLoss()\n",
        "\n",
        "def configure_optimizer(model):\n",
        "  return torch.optim.Adam(model.parameters(), lr = 0.0001)"
      ],
      "metadata": {
        "id": "XqZXP8QDVVS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.from_numpy(X_train.astype(np.float32)) # Numpy to Tensor for PyTorch\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32)).reshape(-1,1)\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32)).reshape(-1,1)"
      ],
      "metadata": {
        "id": "EX8sCT1rrFcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = configure_loss_function()\n",
        "optimizer = configure_optimizer(model)"
      ],
      "metadata": {
        "id": "SucWyr6TAcbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_gd(model, criterion, optimizer, X_train, y_train, epochs = 5000):\n",
        "  train_losses = np.zeros(epochs)\n",
        "  test_losses = np.zeros(epochs)\n",
        "  \n",
        "  for it in range(epochs):\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    outputs_test = model(X_test)\n",
        "    loss_test = criterion(outputs_test, y_test)\n",
        "    \n",
        "    train_losses[it] = loss.item()\n",
        "    test_losses[it] = loss_test.item()\n",
        "    \n",
        "    if (it + 1) % 20 == 0:\n",
        "      print(f\"Epoch {it+1}/{epochs}, Training Loss: {loss.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n",
        "  return train_losses, test_losses"
      ],
      "metadata": {
        "id": "gbq0ZMQ8VhtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, test_losses = full_gd(model, criterion, optimizer, X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gbVqw8EqGN_",
        "outputId": "f1d3ff86-b02b-40c3-9f52-795c334285a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/5000, Training Loss: 0.0437, Test Loss: 0.0536\n",
            "Epoch 40/5000, Training Loss: 0.0436, Test Loss: 0.0534\n",
            "Epoch 60/5000, Training Loss: 0.0434, Test Loss: 0.0533\n",
            "Epoch 80/5000, Training Loss: 0.0433, Test Loss: 0.0532\n",
            "Epoch 100/5000, Training Loss: 0.0432, Test Loss: 0.0531\n",
            "Epoch 120/5000, Training Loss: 0.0431, Test Loss: 0.0530\n",
            "Epoch 140/5000, Training Loss: 0.0430, Test Loss: 0.0529\n",
            "Epoch 160/5000, Training Loss: 0.0430, Test Loss: 0.0527\n",
            "Epoch 180/5000, Training Loss: 0.0429, Test Loss: 0.0526\n",
            "Epoch 200/5000, Training Loss: 0.0428, Test Loss: 0.0525\n",
            "Epoch 220/5000, Training Loss: 0.0427, Test Loss: 0.0524\n",
            "Epoch 240/5000, Training Loss: 0.0426, Test Loss: 0.0523\n",
            "Epoch 260/5000, Training Loss: 0.0425, Test Loss: 0.0522\n",
            "Epoch 280/5000, Training Loss: 0.0424, Test Loss: 0.0521\n",
            "Epoch 300/5000, Training Loss: 0.0424, Test Loss: 0.0520\n",
            "Epoch 320/5000, Training Loss: 0.0423, Test Loss: 0.0519\n",
            "Epoch 340/5000, Training Loss: 0.0422, Test Loss: 0.0518\n",
            "Epoch 360/5000, Training Loss: 0.0421, Test Loss: 0.0518\n",
            "Epoch 380/5000, Training Loss: 0.0420, Test Loss: 0.0517\n",
            "Epoch 400/5000, Training Loss: 0.0419, Test Loss: 0.0516\n",
            "Epoch 420/5000, Training Loss: 0.0419, Test Loss: 0.0515\n",
            "Epoch 440/5000, Training Loss: 0.0418, Test Loss: 0.0514\n",
            "Epoch 460/5000, Training Loss: 0.0417, Test Loss: 0.0513\n",
            "Epoch 480/5000, Training Loss: 0.0416, Test Loss: 0.0512\n",
            "Epoch 500/5000, Training Loss: 0.0416, Test Loss: 0.0511\n",
            "Epoch 520/5000, Training Loss: 0.0415, Test Loss: 0.0510\n",
            "Epoch 540/5000, Training Loss: 0.0414, Test Loss: 0.0510\n",
            "Epoch 560/5000, Training Loss: 0.0414, Test Loss: 0.0509\n",
            "Epoch 580/5000, Training Loss: 0.0413, Test Loss: 0.0508\n",
            "Epoch 600/5000, Training Loss: 0.0412, Test Loss: 0.0507\n",
            "Epoch 620/5000, Training Loss: 0.0412, Test Loss: 0.0506\n",
            "Epoch 640/5000, Training Loss: 0.0411, Test Loss: 0.0506\n",
            "Epoch 660/5000, Training Loss: 0.0410, Test Loss: 0.0505\n",
            "Epoch 680/5000, Training Loss: 0.0409, Test Loss: 0.0504\n",
            "Epoch 700/5000, Training Loss: 0.0409, Test Loss: 0.0503\n",
            "Epoch 720/5000, Training Loss: 0.0408, Test Loss: 0.0502\n",
            "Epoch 740/5000, Training Loss: 0.0407, Test Loss: 0.0502\n",
            "Epoch 760/5000, Training Loss: 0.0407, Test Loss: 0.0501\n",
            "Epoch 780/5000, Training Loss: 0.0406, Test Loss: 0.0500\n",
            "Epoch 800/5000, Training Loss: 0.0405, Test Loss: 0.0499\n",
            "Epoch 820/5000, Training Loss: 0.0405, Test Loss: 0.0498\n",
            "Epoch 840/5000, Training Loss: 0.0404, Test Loss: 0.0497\n",
            "Epoch 860/5000, Training Loss: 0.0403, Test Loss: 0.0497\n",
            "Epoch 880/5000, Training Loss: 0.0402, Test Loss: 0.0496\n",
            "Epoch 900/5000, Training Loss: 0.0402, Test Loss: 0.0495\n",
            "Epoch 920/5000, Training Loss: 0.0401, Test Loss: 0.0494\n",
            "Epoch 940/5000, Training Loss: 0.0400, Test Loss: 0.0493\n",
            "Epoch 960/5000, Training Loss: 0.0399, Test Loss: 0.0492\n",
            "Epoch 980/5000, Training Loss: 0.0399, Test Loss: 0.0491\n",
            "Epoch 1000/5000, Training Loss: 0.0398, Test Loss: 0.0491\n",
            "Epoch 1020/5000, Training Loss: 0.0397, Test Loss: 0.0490\n",
            "Epoch 1040/5000, Training Loss: 0.0397, Test Loss: 0.0489\n",
            "Epoch 1060/5000, Training Loss: 0.0396, Test Loss: 0.0488\n",
            "Epoch 1080/5000, Training Loss: 0.0395, Test Loss: 0.0488\n",
            "Epoch 1100/5000, Training Loss: 0.0395, Test Loss: 0.0487\n",
            "Epoch 1120/5000, Training Loss: 0.0394, Test Loss: 0.0486\n",
            "Epoch 1140/5000, Training Loss: 0.0393, Test Loss: 0.0485\n",
            "Epoch 1160/5000, Training Loss: 0.0393, Test Loss: 0.0485\n",
            "Epoch 1180/5000, Training Loss: 0.0392, Test Loss: 0.0484\n",
            "Epoch 1200/5000, Training Loss: 0.0391, Test Loss: 0.0483\n",
            "Epoch 1220/5000, Training Loss: 0.0391, Test Loss: 0.0482\n",
            "Epoch 1240/5000, Training Loss: 0.0390, Test Loss: 0.0482\n",
            "Epoch 1260/5000, Training Loss: 0.0389, Test Loss: 0.0481\n",
            "Epoch 1280/5000, Training Loss: 0.0389, Test Loss: 0.0480\n",
            "Epoch 1300/5000, Training Loss: 0.0388, Test Loss: 0.0480\n",
            "Epoch 1320/5000, Training Loss: 0.0387, Test Loss: 0.0479\n",
            "Epoch 1340/5000, Training Loss: 0.0387, Test Loss: 0.0478\n",
            "Epoch 1360/5000, Training Loss: 0.0386, Test Loss: 0.0478\n",
            "Epoch 1380/5000, Training Loss: 0.0385, Test Loss: 0.0477\n",
            "Epoch 1400/5000, Training Loss: 0.0385, Test Loss: 0.0476\n",
            "Epoch 1420/5000, Training Loss: 0.0384, Test Loss: 0.0475\n",
            "Epoch 1440/5000, Training Loss: 0.0383, Test Loss: 0.0474\n",
            "Epoch 1460/5000, Training Loss: 0.0383, Test Loss: 0.0473\n",
            "Epoch 1480/5000, Training Loss: 0.0382, Test Loss: 0.0472\n",
            "Epoch 1500/5000, Training Loss: 0.0381, Test Loss: 0.0472\n",
            "Epoch 1520/5000, Training Loss: 0.0380, Test Loss: 0.0471\n",
            "Epoch 1540/5000, Training Loss: 0.0380, Test Loss: 0.0470\n",
            "Epoch 1560/5000, Training Loss: 0.0379, Test Loss: 0.0469\n",
            "Epoch 1580/5000, Training Loss: 0.0378, Test Loss: 0.0468\n",
            "Epoch 1600/5000, Training Loss: 0.0378, Test Loss: 0.0468\n",
            "Epoch 1620/5000, Training Loss: 0.0377, Test Loss: 0.0467\n",
            "Epoch 1640/5000, Training Loss: 0.0376, Test Loss: 0.0466\n",
            "Epoch 1660/5000, Training Loss: 0.0376, Test Loss: 0.0465\n",
            "Epoch 1680/5000, Training Loss: 0.0375, Test Loss: 0.0465\n",
            "Epoch 1700/5000, Training Loss: 0.0374, Test Loss: 0.0464\n",
            "Epoch 1720/5000, Training Loss: 0.0374, Test Loss: 0.0463\n",
            "Epoch 1740/5000, Training Loss: 0.0373, Test Loss: 0.0462\n",
            "Epoch 1760/5000, Training Loss: 0.0372, Test Loss: 0.0462\n",
            "Epoch 1780/5000, Training Loss: 0.0372, Test Loss: 0.0461\n",
            "Epoch 1800/5000, Training Loss: 0.0371, Test Loss: 0.0460\n",
            "Epoch 1820/5000, Training Loss: 0.0370, Test Loss: 0.0460\n",
            "Epoch 1840/5000, Training Loss: 0.0370, Test Loss: 0.0459\n",
            "Epoch 1860/5000, Training Loss: 0.0369, Test Loss: 0.0458\n",
            "Epoch 1880/5000, Training Loss: 0.0369, Test Loss: 0.0458\n",
            "Epoch 1900/5000, Training Loss: 0.0368, Test Loss: 0.0457\n",
            "Epoch 1920/5000, Training Loss: 0.0368, Test Loss: 0.0456\n",
            "Epoch 1940/5000, Training Loss: 0.0367, Test Loss: 0.0456\n",
            "Epoch 1960/5000, Training Loss: 0.0366, Test Loss: 0.0455\n",
            "Epoch 1980/5000, Training Loss: 0.0366, Test Loss: 0.0454\n",
            "Epoch 2000/5000, Training Loss: 0.0365, Test Loss: 0.0454\n",
            "Epoch 2020/5000, Training Loss: 0.0365, Test Loss: 0.0453\n",
            "Epoch 2040/5000, Training Loss: 0.0364, Test Loss: 0.0452\n",
            "Epoch 2060/5000, Training Loss: 0.0363, Test Loss: 0.0452\n",
            "Epoch 2080/5000, Training Loss: 0.0363, Test Loss: 0.0451\n",
            "Epoch 2100/5000, Training Loss: 0.0362, Test Loss: 0.0450\n",
            "Epoch 2120/5000, Training Loss: 0.0362, Test Loss: 0.0450\n",
            "Epoch 2140/5000, Training Loss: 0.0361, Test Loss: 0.0449\n",
            "Epoch 2160/5000, Training Loss: 0.0361, Test Loss: 0.0448\n",
            "Epoch 2180/5000, Training Loss: 0.0360, Test Loss: 0.0448\n",
            "Epoch 2200/5000, Training Loss: 0.0359, Test Loss: 0.0447\n",
            "Epoch 2220/5000, Training Loss: 0.0359, Test Loss: 0.0446\n",
            "Epoch 2240/5000, Training Loss: 0.0358, Test Loss: 0.0446\n",
            "Epoch 2260/5000, Training Loss: 0.0358, Test Loss: 0.0445\n",
            "Epoch 2280/5000, Training Loss: 0.0357, Test Loss: 0.0444\n",
            "Epoch 2300/5000, Training Loss: 0.0357, Test Loss: 0.0444\n",
            "Epoch 2320/5000, Training Loss: 0.0356, Test Loss: 0.0443\n",
            "Epoch 2340/5000, Training Loss: 0.0356, Test Loss: 0.0442\n",
            "Epoch 2360/5000, Training Loss: 0.0355, Test Loss: 0.0442\n",
            "Epoch 2380/5000, Training Loss: 0.0355, Test Loss: 0.0441\n",
            "Epoch 2400/5000, Training Loss: 0.0354, Test Loss: 0.0441\n",
            "Epoch 2420/5000, Training Loss: 0.0354, Test Loss: 0.0440\n",
            "Epoch 2440/5000, Training Loss: 0.0353, Test Loss: 0.0439\n",
            "Epoch 2460/5000, Training Loss: 0.0352, Test Loss: 0.0439\n",
            "Epoch 2480/5000, Training Loss: 0.0352, Test Loss: 0.0438\n",
            "Epoch 2500/5000, Training Loss: 0.0351, Test Loss: 0.0437\n",
            "Epoch 2520/5000, Training Loss: 0.0351, Test Loss: 0.0437\n",
            "Epoch 2540/5000, Training Loss: 0.0350, Test Loss: 0.0436\n",
            "Epoch 2560/5000, Training Loss: 0.0350, Test Loss: 0.0436\n",
            "Epoch 2580/5000, Training Loss: 0.0349, Test Loss: 0.0435\n",
            "Epoch 2600/5000, Training Loss: 0.0349, Test Loss: 0.0434\n",
            "Epoch 2620/5000, Training Loss: 0.0348, Test Loss: 0.0434\n",
            "Epoch 2640/5000, Training Loss: 0.0348, Test Loss: 0.0433\n",
            "Epoch 2660/5000, Training Loss: 0.0347, Test Loss: 0.0433\n",
            "Epoch 2680/5000, Training Loss: 0.0347, Test Loss: 0.0432\n",
            "Epoch 2700/5000, Training Loss: 0.0346, Test Loss: 0.0431\n",
            "Epoch 2720/5000, Training Loss: 0.0346, Test Loss: 0.0431\n",
            "Epoch 2740/5000, Training Loss: 0.0345, Test Loss: 0.0430\n",
            "Epoch 2760/5000, Training Loss: 0.0344, Test Loss: 0.0430\n",
            "Epoch 2780/5000, Training Loss: 0.0344, Test Loss: 0.0429\n",
            "Epoch 2800/5000, Training Loss: 0.0343, Test Loss: 0.0428\n",
            "Epoch 2820/5000, Training Loss: 0.0343, Test Loss: 0.0428\n",
            "Epoch 2840/5000, Training Loss: 0.0342, Test Loss: 0.0427\n",
            "Epoch 2860/5000, Training Loss: 0.0342, Test Loss: 0.0427\n",
            "Epoch 2880/5000, Training Loss: 0.0341, Test Loss: 0.0426\n",
            "Epoch 2900/5000, Training Loss: 0.0341, Test Loss: 0.0426\n",
            "Epoch 2920/5000, Training Loss: 0.0340, Test Loss: 0.0425\n",
            "Epoch 2940/5000, Training Loss: 0.0339, Test Loss: 0.0425\n",
            "Epoch 2960/5000, Training Loss: 0.0339, Test Loss: 0.0424\n",
            "Epoch 2980/5000, Training Loss: 0.0338, Test Loss: 0.0423\n",
            "Epoch 3000/5000, Training Loss: 0.0338, Test Loss: 0.0423\n",
            "Epoch 3020/5000, Training Loss: 0.0337, Test Loss: 0.0422\n",
            "Epoch 3040/5000, Training Loss: 0.0336, Test Loss: 0.0422\n",
            "Epoch 3060/5000, Training Loss: 0.0336, Test Loss: 0.0421\n",
            "Epoch 3080/5000, Training Loss: 0.0335, Test Loss: 0.0420\n",
            "Epoch 3100/5000, Training Loss: 0.0335, Test Loss: 0.0420\n",
            "Epoch 3120/5000, Training Loss: 0.0334, Test Loss: 0.0419\n",
            "Epoch 3140/5000, Training Loss: 0.0333, Test Loss: 0.0419\n",
            "Epoch 3160/5000, Training Loss: 0.0333, Test Loss: 0.0418\n",
            "Epoch 3180/5000, Training Loss: 0.0332, Test Loss: 0.0418\n",
            "Epoch 3200/5000, Training Loss: 0.0332, Test Loss: 0.0417\n",
            "Epoch 3220/5000, Training Loss: 0.0331, Test Loss: 0.0417\n",
            "Epoch 3240/5000, Training Loss: 0.0331, Test Loss: 0.0417\n",
            "Epoch 3260/5000, Training Loss: 0.0330, Test Loss: 0.0416\n",
            "Epoch 3280/5000, Training Loss: 0.0330, Test Loss: 0.0416\n",
            "Epoch 3300/5000, Training Loss: 0.0329, Test Loss: 0.0415\n",
            "Epoch 3320/5000, Training Loss: 0.0329, Test Loss: 0.0415\n",
            "Epoch 3340/5000, Training Loss: 0.0328, Test Loss: 0.0415\n",
            "Epoch 3360/5000, Training Loss: 0.0328, Test Loss: 0.0414\n",
            "Epoch 3380/5000, Training Loss: 0.0327, Test Loss: 0.0414\n",
            "Epoch 3400/5000, Training Loss: 0.0327, Test Loss: 0.0413\n",
            "Epoch 3420/5000, Training Loss: 0.0326, Test Loss: 0.0413\n",
            "Epoch 3440/5000, Training Loss: 0.0326, Test Loss: 0.0413\n",
            "Epoch 3460/5000, Training Loss: 0.0325, Test Loss: 0.0412\n",
            "Epoch 3480/5000, Training Loss: 0.0325, Test Loss: 0.0412\n",
            "Epoch 3500/5000, Training Loss: 0.0324, Test Loss: 0.0411\n",
            "Epoch 3520/5000, Training Loss: 0.0324, Test Loss: 0.0411\n",
            "Epoch 3540/5000, Training Loss: 0.0323, Test Loss: 0.0411\n",
            "Epoch 3560/5000, Training Loss: 0.0323, Test Loss: 0.0410\n",
            "Epoch 3580/5000, Training Loss: 0.0322, Test Loss: 0.0410\n",
            "Epoch 3600/5000, Training Loss: 0.0322, Test Loss: 0.0409\n",
            "Epoch 3620/5000, Training Loss: 0.0321, Test Loss: 0.0409\n",
            "Epoch 3640/5000, Training Loss: 0.0321, Test Loss: 0.0409\n",
            "Epoch 3660/5000, Training Loss: 0.0320, Test Loss: 0.0408\n",
            "Epoch 3680/5000, Training Loss: 0.0320, Test Loss: 0.0408\n",
            "Epoch 3700/5000, Training Loss: 0.0319, Test Loss: 0.0407\n",
            "Epoch 3720/5000, Training Loss: 0.0319, Test Loss: 0.0407\n",
            "Epoch 3740/5000, Training Loss: 0.0318, Test Loss: 0.0407\n",
            "Epoch 3760/5000, Training Loss: 0.0318, Test Loss: 0.0406\n",
            "Epoch 3780/5000, Training Loss: 0.0317, Test Loss: 0.0406\n",
            "Epoch 3800/5000, Training Loss: 0.0317, Test Loss: 0.0406\n",
            "Epoch 3820/5000, Training Loss: 0.0316, Test Loss: 0.0405\n",
            "Epoch 3840/5000, Training Loss: 0.0316, Test Loss: 0.0405\n",
            "Epoch 3860/5000, Training Loss: 0.0315, Test Loss: 0.0404\n",
            "Epoch 3880/5000, Training Loss: 0.0315, Test Loss: 0.0404\n",
            "Epoch 3900/5000, Training Loss: 0.0314, Test Loss: 0.0404\n",
            "Epoch 3920/5000, Training Loss: 0.0313, Test Loss: 0.0404\n",
            "Epoch 3940/5000, Training Loss: 0.0313, Test Loss: 0.0403\n",
            "Epoch 3960/5000, Training Loss: 0.0312, Test Loss: 0.0403\n",
            "Epoch 3980/5000, Training Loss: 0.0312, Test Loss: 0.0403\n",
            "Epoch 4000/5000, Training Loss: 0.0311, Test Loss: 0.0402\n",
            "Epoch 4020/5000, Training Loss: 0.0310, Test Loss: 0.0402\n",
            "Epoch 4040/5000, Training Loss: 0.0309, Test Loss: 0.0401\n",
            "Epoch 4060/5000, Training Loss: 0.0309, Test Loss: 0.0401\n",
            "Epoch 4080/5000, Training Loss: 0.0308, Test Loss: 0.0400\n",
            "Epoch 4100/5000, Training Loss: 0.0307, Test Loss: 0.0400\n",
            "Epoch 4120/5000, Training Loss: 0.0307, Test Loss: 0.0400\n",
            "Epoch 4140/5000, Training Loss: 0.0306, Test Loss: 0.0399\n",
            "Epoch 4160/5000, Training Loss: 0.0306, Test Loss: 0.0399\n",
            "Epoch 4180/5000, Training Loss: 0.0305, Test Loss: 0.0399\n",
            "Epoch 4200/5000, Training Loss: 0.0304, Test Loss: 0.0399\n",
            "Epoch 4220/5000, Training Loss: 0.0304, Test Loss: 0.0398\n",
            "Epoch 4240/5000, Training Loss: 0.0303, Test Loss: 0.0398\n",
            "Epoch 4260/5000, Training Loss: 0.0303, Test Loss: 0.0398\n",
            "Epoch 4280/5000, Training Loss: 0.0302, Test Loss: 0.0397\n",
            "Epoch 4300/5000, Training Loss: 0.0302, Test Loss: 0.0397\n",
            "Epoch 4320/5000, Training Loss: 0.0301, Test Loss: 0.0397\n",
            "Epoch 4340/5000, Training Loss: 0.0301, Test Loss: 0.0396\n",
            "Epoch 4360/5000, Training Loss: 0.0300, Test Loss: 0.0396\n",
            "Epoch 4380/5000, Training Loss: 0.0299, Test Loss: 0.0395\n",
            "Epoch 4400/5000, Training Loss: 0.0299, Test Loss: 0.0395\n",
            "Epoch 4420/5000, Training Loss: 0.0298, Test Loss: 0.0395\n",
            "Epoch 4440/5000, Training Loss: 0.0298, Test Loss: 0.0394\n",
            "Epoch 4460/5000, Training Loss: 0.0297, Test Loss: 0.0394\n",
            "Epoch 4480/5000, Training Loss: 0.0297, Test Loss: 0.0393\n",
            "Epoch 4500/5000, Training Loss: 0.0296, Test Loss: 0.0393\n",
            "Epoch 4520/5000, Training Loss: 0.0295, Test Loss: 0.0392\n",
            "Epoch 4540/5000, Training Loss: 0.0295, Test Loss: 0.0391\n",
            "Epoch 4560/5000, Training Loss: 0.0294, Test Loss: 0.0391\n",
            "Epoch 4580/5000, Training Loss: 0.0293, Test Loss: 0.0390\n",
            "Epoch 4600/5000, Training Loss: 0.0293, Test Loss: 0.0389\n",
            "Epoch 4620/5000, Training Loss: 0.0292, Test Loss: 0.0389\n",
            "Epoch 4640/5000, Training Loss: 0.0292, Test Loss: 0.0389\n",
            "Epoch 4660/5000, Training Loss: 0.0291, Test Loss: 0.0388\n",
            "Epoch 4680/5000, Training Loss: 0.0290, Test Loss: 0.0388\n",
            "Epoch 4700/5000, Training Loss: 0.0290, Test Loss: 0.0387\n",
            "Epoch 4720/5000, Training Loss: 0.0289, Test Loss: 0.0387\n",
            "Epoch 4740/5000, Training Loss: 0.0289, Test Loss: 0.0386\n",
            "Epoch 4760/5000, Training Loss: 0.0288, Test Loss: 0.0386\n",
            "Epoch 4780/5000, Training Loss: 0.0288, Test Loss: 0.0385\n",
            "Epoch 4800/5000, Training Loss: 0.0287, Test Loss: 0.0385\n",
            "Epoch 4820/5000, Training Loss: 0.0287, Test Loss: 0.0384\n",
            "Epoch 4840/5000, Training Loss: 0.0286, Test Loss: 0.0384\n",
            "Epoch 4860/5000, Training Loss: 0.0286, Test Loss: 0.0383\n",
            "Epoch 4880/5000, Training Loss: 0.0285, Test Loss: 0.0383\n",
            "Epoch 4900/5000, Training Loss: 0.0285, Test Loss: 0.0383\n",
            "Epoch 4920/5000, Training Loss: 0.0284, Test Loss: 0.0382\n",
            "Epoch 4940/5000, Training Loss: 0.0284, Test Loss: 0.0382\n",
            "Epoch 4960/5000, Training Loss: 0.0283, Test Loss: 0.0381\n",
            "Epoch 4980/5000, Training Loss: 0.0282, Test Loss: 0.0381\n",
            "Epoch 5000/5000, Training Loss: 0.0282, Test Loss: 0.0381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  p_train = model(X_train)\n",
        "  p_train = (p_train.numpy() > 0)\n",
        "\n",
        "  train_acc = np.mean(y_train.numpy() == p_train)\n",
        "\n",
        "  p_test = model(X_test)\n",
        "  p_test = (p_test.numpy() > 0)\n",
        "\n",
        "  test_acc = np.mean(y_test.numpy() == p_test)\n",
        "\n",
        "print(f'Training Accuracy is: {train_acc*100:.2f}%')\n",
        "print(f'Testing Accuracy is: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "skKd9uDaLVMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53c0737-b29c-4b83-e6cb-ae06c89dde75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy is: 98.92%\n",
            "Testing Accuracy is: 98.61%\n"
          ]
        }
      ]
    }
  ]
}