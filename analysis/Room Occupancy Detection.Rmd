---
title: "Room Occupancy Detection"
author: "Charles Stuppard, Mergen Narangerel"
date: "12/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      include = FALSE,
                      fig.show = "hold",
                      out.width = "50%",
                      message = F)
library(tidyverse)
library(tidymodels)
library(doParallel)
library(vip)
```

```{r reading}
occupancy <- read.csv("../data/occupancy_data.csv") %>%
  mutate(Occupancy = as_factor(Occupancy))
```

```{r wrangling}
occupancy %>%
  count(Occupancy)

occupancy %>%
  select(where(is.numeric)) %>%
  cor(use = 'complete.obs', method = "pearson")
```

```{r skimmer}
skimr::skim(occupancy)
```

```{r plotting}
occupancy %>%
  ggplot(aes(x = Temperature, color = Occupancy))+
  geom_freqpoly(binwidth = 0.2)+
  theme_minimal()

occupancy %>%
  ggplot(aes(x = CO2, color = Occupancy))+
  geom_freqpoly(binwidth = 10)+
  theme_minimal()

occupancy %>%
  ggplot(aes(x = Light, color = Occupancy))+
  geom_freqpoly(binwidth = 10)+
  theme_minimal()

occupancy %>%
  ggplot(aes(x = Humidity, color = Occupancy))+
  geom_freqpoly(binwidth = 0.2)+
  theme_minimal()
```

```{r splitting}
set.seed(123)

occ <- occupancy %>%
  select(-date)

occ_split <- initial_split(occ)
occ_train <- training(occ_split)
occ_test <- testing(occ_split)

occ_folds <- vfold_cv(occ_train, v = 10, strata = Occupancy)
```

```{r log recipe}
log_rec <- recipe(Occupancy ~ ., data = occ_train)

occ_log <- occ_train %>%
  mutate(log_temp = log(Temperature+1),
         log_humid = log(Humidity+1),
         log_light = log(Light+1),
         log_co2 = log(CO2+1))
```

```{r log spec}
log_fit <- glm(Occupancy ~ Temperature + Humidity + Light + CO2 + 
                 Temperature:log_temp + Humidity:log_humid + 
                 Light:log_light + CO2:log_co2,
               family = binomial(link = 'logit'), data = occ_log)

summary(log_fit)
```

```{r svm rec}
svm_rec <- recipe(Occupancy ~ ., data = occ_train) %>% 
  step_normalize(all_numeric_predictors())

```

```{r svm spec}
svm_lin_spec <- svm_linear() %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```

```{r svm wflow}
svm_wf <- workflow(svm_rec, svm_lin_spec) 
```

```{r svm fitting}
svm_fit <- fit(svm_wf, data = occ_train)
```

```{r svm results}

svm_fit %>%
  augment(new_data = occ_train) %>%
  conf_mat(Occupancy, .pred_class) %>%
  autoplot(type = "heatmap") +
  scale_fill_gradient(low = 'pink', high = 'royalblue')

augment(svm_fit, occ_train) %>%
  accuracy(truth = Occupancy, estimate = .pred_class)


```



```{r xgb rec }
xgb_rec <- recipe(Occupancy ~ ., data = occ_train)
```

```{r xgb spec}
require(xgboost)
xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune(),
  learn_rate = tune()
) %>%
  set_engine('xgboost') %>%
  set_mode('classification')

xgb_spec
```

```{r xgb wflow}
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), occ_train),
  learn_rate(),
  size = 30
)

xgb_wflow <- workflow() %>%
  add_recipe(xgb_rec) %>%
  add_model(xgb_spec)

xgb_wflow
```

```{r xgb fitting, cache = TRUE, catch.path = "cache/"}
cl_mergen <- makePSOCKcluster(detectCores())
registerDoParallel(cl_mergen)

set.seed(200)
xgb_fit <- tune_grid(
  xgb_wflow,
  resamples = occ_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)
```

```{r xgb results}
xgb_fit %>%
  collect_metrics() %>%
  filter(.metric == 'roc_auc') %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = 'value',
               names_to = 'parameter') %>%
  ggplot(aes(x = value, y = mean, color = parameter)) +
  geom_point(alpha = 0.7, show.legend = FALSE)+
  facet_wrap(~parameter, scales = 'free_x') +
  labs(x = NULL, y = 'Area Under Curve')+
  theme_minimal()
```

```{r showing results}
xgb_fit %>%
  show_best(metric = 'accuracy')

xgb_best <- xgb_fit %>%
  select_best('accuracy')

xgb_final <- finalize_workflow(xgb_wflow, xgb_best)

xgb_final %>%
  fit(data = occ_train) %>%
  augment(new_data = occ_train) %>%
  conf_mat(truth = Occupancy, estimate = .pred_class) %>%
  autoplot('heatmap') +
  scale_fill_gradient(low = 'pink', high = 'royalblue')

xgb_final %>%
  fit(data = occ_train) %>%
  extract_fit_parsnip() %>%
  vip(geom = 'point')+
  theme_minimal()

xgb_final
```

```{r test set}
xgb_final_test <- last_fit(xgb_final, occ_split)
```

```{r test visual}
collect_metrics(xgb_final_test)


xgb_final_test$.predictions[[1]] %>%
  conf_mat(Occupancy, .pred_class) %>%
  autoplot('heatmap')+
  scale_fill_gradient(low = 'pink', high = 'royalblue')
```


#### **Introduction**

In general, occupancy sensors use a collection of sensory information to detect 
the presence of a person. Such sensors operate on different measurements such 
as: light illumination, temperature and energy (electricity) output. 
Industrial applications of such sensors range from home security, 
energy savings and automation. 

Our research uses recorded data of light illumination, humidity, CO2 and 
temperature to detect human occupancy through the use of supervised 
classification models. The data has been trained and tested using 
“Extreme Gradient Boosting” often referred as “xgboost”. 
The model optimizes loss functions by using gradient descent which moves in 
the direction of steepest (thus most efficient) descent which minimizes the 
error of ground truth vs prediction (Chong, Zak p131). 

From the given dataset `xgboost` was able to produce high performing results. 

#### **Objective**

The objective of our model is to be able to accurately detect the presence of a
person given updated information from the sensors on the test data. 

#### **Data**

The original dataset was obtained from the UCI Machine Learning Repository with
7 features and no missing variables. 

```{r ref1, ref.label = "wrangling", include = TRUE}

```

```{r ref2, ref.label = "skimmer", include = TRUE}

```


We do note high association between our independent variables. Such as the 
association between `Humidity` and `HumidityRatio`. However, Gradient Boosted
Trees are known to be robust to multicollinearity. Moreover, our model 
conclusions will indicate stable output irrespective of the association. 

```{r ref3, ref.label = "plotting", include = TRUE}

```


Based on the initial plots we do see that `Light` or illumination has a clear
separation between occupancy states `[0,1]`. And thus it will be a strong 
predictor. `CO2` and `Temperature` seem to have some distinctions between 
occupancy states while `Humidity` has no distinction. 


#### **Methods**

Our baseline approach was a binary logistic regression. However it did not
pass the Box-Tidwell test. Thus not satisfying the linearity of logodds \
assumption. 

```{r ref4, ref.label= "log spec", include = TRUE}

```


We split the data into training and testing with 10 fold stratified cross
validation samples.

```{r ref5, ref.label = "xgb rec", include = TRUE}

```


We are tuning most of our hyperparameters inside an exhaustive latin hypercube
sampling method. We were mindful of overfitting whilst tuning `tree_depth`. 

```{r ref6, ref.label = "xgb spec", include = TRUE}

```


```{r ref7, ref.label = "xgb wflow"}

```


#### **Results**

The SVM model correctly predicted 98.9% of variables. However, it was the lowest
performing of the models used:

```{r ref8, ref.label = "svm results", include = TRUE}

```

The `xgboost` model was fitted to the training set: 

```{r ref9, ref.label = "xgb results", include = TRUE}

```

The model performance improved upon the svm model. `Light` is determined to be 
the most impactful predictor. 

```{r ref10, ref.label = "showing results", include = TRUE, warning=FALSE}

```

Lastly, the best `xgboost` model was fitted to the training data, resulting in 
99.1% prediction accuracy:

```{r ref11, ref.label = "test visual", include = TRUE}

```


#### **Reflection**

The `svm` and `xgboost` models performed well on the given dataset with an 
estimated error rate under 3%. Testing the models on a larger, scalable data 
set would be beneficial in attempting to further optimize the model, although 
the results would likely hold. There are several possibilities for next steps 
and future research. An important consideration is the application of the room 
sensors and their sensitivity requirement. Further research could explore the 
minimum and/or maximum values that trigger detection and determine the threshold
of what classifies as a person, as opposed to a child or large animal. `Light` 
was the most important factor, so it may be worth looking into how the sensors 
perform in the absence of light. Alternatively, would the models perform the 
same with *only* the light variable. Due to time constraints, neural networks 
and deep learning classification methods were not used. 


#### **References**

- Edwin K. P. Chong, Stanislaw H. Zak (2013). An Introduction to Optimization 
(4th Ed.). John Wiley & Sons 
https://books.google.com/books/about/An_Introduction_to_Optimization.html?id=8J_ev5ihKEoC

- Chen T., Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. DOI 
https://dl.acm.org/doi/10.1145/2939672.2939785

#### **Appendix**

```{r show-code, ref.label= knitr::all_labels(), echo=TRUE, eval=FALSE, include=TRUE}

```
